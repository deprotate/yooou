{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Cleaned Age. NaNs count: 97\n",
      "Loading cached BERT embeddings...\n",
      "Finding optimal iterations and calculating NDCG...\n",
      "Training SVD...\n",
      "Building features...\n",
      "Generating Audience Demographics features...\n",
      "Building features...\n",
      "Generating Audience Demographics features...\n",
      ">>> CatBoost - Best Iteration: 54, Mean NDCG@20 on Val: 0.9986\n",
      ">>> XGBoost - Best Iteration: 3, Mean NDCG@20 on Val: 0.9972\n",
      "\n",
      "============================== ENSEMBLE TRAINING ==============================\n",
      "Training SVD...\n",
      "Building features...\n",
      "Generating Audience Demographics features...\n",
      "Building features...\n",
      "Generating Audience Demographics features...\n",
      "Training Ensemble Models 1/5 with seed 42...\n",
      "Training Ensemble Models 2/5 with seed 1337...\n",
      "Training Ensemble Models 3/5 with seed 777...\n",
      "Training Ensemble Models 4/5 with seed 2024...\n",
      "Training Ensemble Models 5/5 with seed 100...\n",
      "Ensemble Done! Saved to output/submissions/sunny_xgb_cb.csv\n",
      ">>> Final Ensemble - Mean NDCG@20 on Validation: 0.9979\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from catboost import CatBoostRanker, Pool\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Torch/Transformers check\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    BERT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "\n",
    "# XGBoost check\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Config:\n",
    "    ROOT_DIR = Path(\".\")\n",
    "    DATA_DIR = ROOT_DIR / \"data\"\n",
    "    RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "    PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "    MODEL_DIR = Path(\"output/models\")\n",
    "    SUBMISSION_DIR = Path(\"output/submissions\")\n",
    "\n",
    "    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ВАЖНО: Мы будем использовать список сидов для ансамбля\n",
    "    SEEDS = [42, 1337, 777, 2024, 100]\n",
    "\n",
    "    NEGATIVES_PER_USER = 15\n",
    "\n",
    "    USE_BERT = True\n",
    "    BERT_MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "    BERT_BATCH_SIZE = 8\n",
    "    BERT_MAX_LEN = 128\n",
    "\n",
    "    VAL_SIZE_RATIO = 0.2\n",
    "\n",
    "    CB_PARAMS = {\n",
    "        'loss_function': 'YetiRank',\n",
    "        'iterations': 3000,  # Уменьшим, раз он и так стопается на 30\n",
    "        'learning_rate': 0.03,\n",
    "        'depth': 6,\n",
    "        'task_type': 'CPU',\n",
    "        'verbose': 0,  # Молчаливый режим для ансамбля\n",
    "        'eval_metric': 'NDCG:top=20',\n",
    "        'early_stopping_rounds': 50\n",
    "    }\n",
    "    \n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'rank:pairwise',\n",
    "        'eval_metric': 'ndcg@20',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_estimators': 3000,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "        'early_stopping_rounds': 80\n",
    "    }\n",
    "\n",
    "\n",
    "class Constants:\n",
    "    TRAIN_FILENAME = \"train.csv\"\n",
    "    TARGETS_FILENAME = \"targets.csv\"\n",
    "    CANDIDATES_FILENAME = \"candidates.csv\"\n",
    "    USER_DATA_FILENAME = \"users.csv\"\n",
    "    BOOK_DATA_FILENAME = \"books.csv\"\n",
    "    BOOK_GENRES_FILENAME = \"book_genres.csv\"\n",
    "    GENRES_FILENAME = \"genres.csv\"\n",
    "    BOOK_DESCRIPTIONS_FILENAME = \"book_descriptions.csv\"\n",
    "\n",
    "    COL_USER_ID = \"user_id\"\n",
    "    COL_BOOK_ID = \"book_id\"\n",
    "    COL_TIMESTAMP = \"timestamp\"\n",
    "    COL_HAS_READ = \"has_read\"\n",
    "    COL_RELEVANCE = \"relevance\"\n",
    "    COL_DESCRIPTION = \"description\"\n",
    "    COL_BOOK_ID_LIST = \"book_id_list\"\n",
    "\n",
    "    F_SVD_SCORE = \"svd_score\"\n",
    "    F_BERT_SIM = \"bert_cosine_sim\"\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if BERT_AVAILABLE:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# --- BERT ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, ids, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.ids = ids\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length',\n",
    "            truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'book_id': torch.tensor(self.ids[item], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_bert_embeddings(desc_df):\n",
    "    if not BERT_AVAILABLE or not Config.USE_BERT: return {}\n",
    "    cache_path = Config.PROCESSED_DATA_DIR / \"bert_embeddings.pkl\"\n",
    "    if cache_path.exists():\n",
    "        print(\"Loading cached BERT embeddings...\")\n",
    "        return joblib.load(cache_path)\n",
    "\n",
    "    # ... (код вычисления BERT опущен, он такой же как в v5, берется из кэша) ...\n",
    "    print(\"Warning: Cache not found, please ensure bert_embeddings.pkl exists from previous run to save time!\")\n",
    "    return {}\n",
    "\n",
    "\n",
    "# --- SVD  ---\n",
    "def train_svd_model(train_df):\n",
    "    print(\"Training SVD...\")\n",
    "    train_df['weight'] = train_df[Constants.COL_HAS_READ].map({1: 2, 0: 1})\n",
    "    users = train_df[Constants.COL_USER_ID].unique()\n",
    "    books = train_df[Constants.COL_BOOK_ID].unique()\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    book_map = {b: i for i, b in enumerate(books)}\n",
    "    row = train_df[Constants.COL_USER_ID].map(user_map).values\n",
    "    col = train_df[Constants.COL_BOOK_ID].map(book_map).values\n",
    "    data = train_df['weight'].values\n",
    "    sparse_matrix = sparse.csr_matrix((data, (row, col)), shape=(len(users), len(books)))\n",
    "    svd = TruncatedSVD(n_components=64, random_state=42)  # Фикс сид для SVD\n",
    "    u_fac = svd.fit_transform(sparse_matrix)\n",
    "    i_fac = svd.components_.T\n",
    "    return u_fac, i_fac, user_map, book_map\n",
    "\n",
    "\n",
    "def get_svd_score(user_ids, book_ids, u_fac, i_fac, u_map, b_map):\n",
    "    u_indices = np.array([u_map.get(u, -1) for u in user_ids])\n",
    "    b_indices = np.array([b_map.get(b, -1) for b in book_ids])\n",
    "    scores = np.zeros(len(user_ids), dtype=np.float32)\n",
    "    mask = (u_indices != -1) & (b_indices != -1)\n",
    "    if mask.sum() > 0:\n",
    "        scores[mask] = np.sum(u_fac[u_indices[mask]] * i_fac[b_indices[mask]], axis=1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# --- NEW FEATURES: AUDIENCE PROFILE ---\n",
    "def add_audience_features(df, train_history_df, user_meta):\n",
    "    print(\"Generating Audience Demographics features...\")\n",
    "\n",
    "    # Мержим историю с юзерами (у юзеров уже почищен возраст, там есть NaN вместо нулей)\n",
    "    history_with_meta = train_history_df.merge(user_meta, on=Constants.COL_USER_ID, how='left')\n",
    "\n",
    "    # Группируем. Pandas mean() АВТОМАТИЧЕСКИ ИГНОРИРУЕТ NaN.\n",
    "    # То есть средний возраст книги посчитается только по тем, кто указал нормальный возраст.\n",
    "    book_audience = history_with_meta.groupby(Constants.COL_BOOK_ID).agg(\n",
    "        book_audience_age_mean=('age', 'mean'),\n",
    "        book_audience_age_std=('age', 'std')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Мержим к текущему датасету\n",
    "    df = df.merge(book_audience, on=Constants.COL_BOOK_ID, how='left')\n",
    "\n",
    "    # Вот теперь заполняем пропуски для тех книг, где вообще никто возраст не указал\n",
    "    # Заполняем глобальной медианой (из чистых данных)\n",
    "    clean_global_median = user_meta['age'].median()\n",
    "\n",
    "    df['book_audience_age_mean'] = df['book_audience_age_mean'].fillna(clean_global_median)\n",
    "    df['book_audience_age_std'] = df['book_audience_age_std'].fillna(10.0)\n",
    "\n",
    "    # Для разницы (diff) нам нужен возраст текущего юзера\n",
    "    if 'age' not in df.columns:\n",
    "        df = df.merge(user_meta[[Constants.COL_USER_ID, 'age']], on=Constants.COL_USER_ID, how='left')\n",
    "\n",
    "    # Если у текущего юзера возраст NaN (кривой), мы не можем посчитать разницу.\n",
    "    # Заполним его возраст тоже медианой, чтобы модель получила хоть какое-то число.\n",
    "    # (Или можно оставить NaN, CatBoost съест, но для diff лучше число)\n",
    "    df['age_filled'] = df['age'].fillna(clean_global_median)\n",
    "\n",
    "    # Считаем разницу\n",
    "    df['age_diff_with_audience'] = abs(df['age_filled'] - df['book_audience_age_mean'])\n",
    "\n",
    "    # Удаляем временную колонку\n",
    "    df = df.drop(columns=['age_filled'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_heuristic_features(df, train_history_df, book_meta):\n",
    "    book_stats = train_history_df.groupby(Constants.COL_BOOK_ID).agg(\n",
    "        book_pop_count=(Constants.COL_USER_ID, 'count'),\n",
    "        book_global_mean=(Constants.COL_HAS_READ, 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    train_with_meta = train_history_df.merge(book_meta[[Constants.COL_BOOK_ID, 'author_id']], on=Constants.COL_BOOK_ID,\n",
    "                                             how='left')\n",
    "    user_author_stats = train_with_meta.groupby([Constants.COL_USER_ID, 'author_id'])[Constants.COL_HAS_READ].agg(\n",
    "        user_author_count='count',\n",
    "        user_author_mean='mean'\n",
    "    ).reset_index()\n",
    "\n",
    "    df = df.merge(book_stats, on=Constants.COL_BOOK_ID, how='left')\n",
    "    if 'author_id' not in df.columns:\n",
    "        df = df.merge(book_meta[[Constants.COL_BOOK_ID, 'author_id']], on=Constants.COL_BOOK_ID, how='left')\n",
    "    df = df.merge(user_author_stats, on=[Constants.COL_USER_ID, 'author_id'], how='left')\n",
    "\n",
    "    df['book_pop_count'] = df['book_pop_count'].fillna(0)\n",
    "    df['book_global_mean'] = df['book_global_mean'].fillna(train_history_df[Constants.COL_HAS_READ].mean())\n",
    "    df['user_author_count'] = df['user_author_count'].fillna(0)\n",
    "    df['user_author_mean'] = df['user_author_mean'].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- BUILD FEATURES ---\n",
    "def build_features(df, u_meta, b_meta, desc_df, svd_data, bert_embs, train_history_full, user_profiles=None):\n",
    "    print(\"Building features...\")\n",
    "    df = df.merge(u_meta, on=Constants.COL_USER_ID, how='left')\n",
    "    cols_to_use = [c for c in b_meta.columns if c not in df.columns or c == Constants.COL_BOOK_ID]\n",
    "    df = df.merge(b_meta[cols_to_use], on=Constants.COL_BOOK_ID, how='left')\n",
    "\n",
    "    u_fac, i_fac, u_map, b_map = svd_data\n",
    "    df[Constants.F_SVD_SCORE] = get_svd_score(df[Constants.COL_USER_ID], df[Constants.COL_BOOK_ID], u_fac, i_fac, u_map,\n",
    "                                              b_map)\n",
    "\n",
    "    # 1. Standard Heuristics\n",
    "    df = add_heuristic_features(df, train_history_full, b_meta)\n",
    "\n",
    "    # 2. NEW: Audience Demographics\n",
    "    df = add_audience_features(df, train_history_full, u_meta)\n",
    "\n",
    "    # 3. BERT Features (Sim + Raw)\n",
    "    if bert_embs:\n",
    "        sample_key = next(iter(bert_embs))\n",
    "        dim = len(bert_embs[sample_key])\n",
    "\n",
    "        # Similarity\n",
    "        if user_profiles:\n",
    "            # Маппинг для векторов \n",
    "            # Упрощенная вставка кода для краткости\n",
    "            # Предполагаем, что user_profiles уже вычислен\n",
    "            u_ids = df[Constants.COL_USER_ID].values\n",
    "            b_ids = df[Constants.COL_BOOK_ID].values\n",
    "            u_vecs = np.array([user_profiles.get(u, np.zeros(dim)) for u in u_ids])\n",
    "            b_vecs = np.array([bert_embs.get(b, np.zeros(dim)) for b in b_ids])\n",
    "\n",
    "            dot = np.sum(u_vecs * b_vecs, axis=1)\n",
    "            n_u = np.linalg.norm(u_vecs, axis=1)\n",
    "            n_b = np.linalg.norm(b_vecs, axis=1)\n",
    "            df[Constants.F_BERT_SIM] = dot / (n_u * n_b + 1e-9)\n",
    "\n",
    "        # Raw BERT (First 64 dims)\n",
    "        dim_raw = 64\n",
    "        emb_matrix = np.zeros((len(df), dim_raw), dtype=np.float32)\n",
    "        b_ids = df[Constants.COL_BOOK_ID].values\n",
    "        for i, bid in enumerate(b_ids):\n",
    "            if bid in bert_embs:\n",
    "                emb_matrix[i] = bert_embs[bid][:dim_raw]\n",
    "        bert_df = pd.DataFrame(emb_matrix, columns=[f\"bert_{i}\" for i in range(dim_raw)], index=df.index)\n",
    "        df = pd.concat([df, bert_df], axis=1)\n",
    "\n",
    "    # Clean\n",
    "    cat_cols = ['gender', 'author_id', 'publisher', 'language']\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns: df[c] = df[c].fillna(\"unk\").astype(str)\n",
    "\n",
    "    num_cols = ['age', 'publication_year', 'avg_rating', 'book_pop_count',\n",
    "                'user_author_count', 'book_audience_age_mean', 'age_diff_with_audience']\n",
    "    for c in num_cols:\n",
    "        if c in df.columns: df[c] = df[c].fillna(0)\n",
    "\n",
    "    return df, cat_cols\n",
    "\n",
    "\n",
    "# --- HELPERS (Load, Negatives, Clean, Expand) - same as v5 ---\n",
    "def load_and_prep():\n",
    "    print(\"Loading data...\")\n",
    "    dtype_spec = {Constants.COL_USER_ID: \"int32\", Constants.COL_BOOK_ID: \"int32\", Constants.COL_HAS_READ: \"int32\"}\n",
    "\n",
    "    train = pd.read_csv(Config.RAW_DATA_DIR / Constants.TRAIN_FILENAME, dtype=dtype_spec,\n",
    "                        parse_dates=[Constants.COL_TIMESTAMP])\n",
    "    train[Constants.COL_RELEVANCE] = train[Constants.COL_HAS_READ].map({1: 2, 0: 1}).astype(\"int8\")\n",
    "\n",
    "    candidates = pd.read_csv(Config.RAW_DATA_DIR / Constants.CANDIDATES_FILENAME,\n",
    "                             dtype={Constants.COL_USER_ID: \"int32\"})\n",
    "\n",
    "    # --- ЗАГРУЗКА И ЧИСТКА ЮЗЕРОВ ---\n",
    "    user_meta = pd.read_csv(Config.RAW_DATA_DIR / Constants.USER_DATA_FILENAME)\n",
    "\n",
    "    # 1. Чистим возраст: всё что меньше 6 и больше 95 превращаем в NaN (пустоту)\n",
    "    if 'age' in user_meta.columns:\n",
    "        user_meta.loc[(user_meta['age'] <= 5) | (user_meta['age'] >= 95), 'age'] = np.nan\n",
    "        print(f\"Cleaned Age. NaNs count: {user_meta['age'].isna().sum()}\")\n",
    "\n",
    "    # 2. Пол (gender). Если он числовой, убедимся что он адекватный, или тоже в NaN/Unknown\n",
    "    # Обычно там 0, 1, 2. Если есть мусор - можно тоже почистить, но возраст важнее.\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    book_meta = pd.read_csv(Config.RAW_DATA_DIR / Constants.BOOK_DATA_FILENAME).drop_duplicates(Constants.COL_BOOK_ID)\n",
    "    book_desc = pd.read_csv(Config.RAW_DATA_DIR / Constants.BOOK_DESCRIPTIONS_FILENAME)\n",
    "\n",
    "    return train, candidates, user_meta, book_meta, book_desc\n",
    "\n",
    "\n",
    "def generate_negatives(train_df, all_books):\n",
    "    user_inter = train_df.groupby(Constants.COL_USER_ID)[Constants.COL_BOOK_ID].apply(set).to_dict()\n",
    "    all_books_arr = np.array(all_books)\n",
    "    rows = []\n",
    "    for uid, books in user_inter.items():\n",
    "        cands = np.random.choice(all_books_arr, size=Config.NEGATIVES_PER_USER + 5)\n",
    "        cnt = 0\n",
    "        for b in cands:\n",
    "            if b not in books:\n",
    "                rows.append({Constants.COL_USER_ID: uid, Constants.COL_BOOK_ID: b, Constants.COL_RELEVANCE: 0})\n",
    "                cnt += 1\n",
    "                if cnt >= Config.NEGATIVES_PER_USER: break\n",
    "    return pd.concat([train_df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "\n",
    "def clean_data_for_models(df, cat_cols):\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    garbage_cols = [c for c in obj_cols if c not in cat_cols]\n",
    "    if garbage_cols: df = df.drop(columns=garbage_cols)\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns: df[c] = df[c].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_user_bert_profiles(train_history_df, bert_embs):\n",
    "    # (Код из v5)\n",
    "    if not bert_embs: return {}, 0\n",
    "    sample_key = next(iter(bert_embs))\n",
    "    dim = len(bert_embs[sample_key])\n",
    "    bert_data = []\n",
    "    for bid, vec in bert_embs.items():\n",
    "        bert_data.append([bid] + list(vec))\n",
    "    bert_df = pd.DataFrame(bert_data, columns=[Constants.COL_BOOK_ID] + [f\"b{i}\" for i in range(dim)])\n",
    "    merged = train_history_df.merge(bert_df, on=Constants.COL_BOOK_ID, how='inner')\n",
    "    user_profiles = merged.groupby(Constants.COL_USER_ID)[[f\"b{i}\" for i in range(dim)]].mean()\n",
    "    return {uid: row.values for uid, row in user_profiles.iterrows()}, dim\n",
    "\n",
    "\n",
    "def expand_candidates(df):\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        if pd.isna(r[Constants.COL_BOOK_ID_LIST]): continue\n",
    "        for b in str(r[Constants.COL_BOOK_ID_LIST]).split(','):\n",
    "            if b.strip(): rows.append((r[Constants.COL_USER_ID], int(b.strip())))\n",
    "    return pd.DataFrame(rows, columns=[Constants.COL_USER_ID, Constants.COL_BOOK_ID])\n",
    "\n",
    "\n",
    "def prepare_group_info(group_ids):\n",
    "    \"\"\"Convert group IDs to group sizes for XGBoost\"\"\"\n",
    "    unique_groups, counts = np.unique(group_ids, return_counts=True)\n",
    "    return counts.tolist()\n",
    "\n",
    "\n",
    "# === Calculating metrics === \n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def calculate_ndcg_at_k(y_true, y_pred, k=20):\n",
    "    if len(y_pred) <= k:\n",
    "        top_k_indices = np.arange(len(y_pred))\n",
    "    else:\n",
    "        top_k_indices = np.argsort(y_pred)[::-1][:k]\n",
    "\n",
    "    y_true_at_k = np.take(y_true, top_k_indices)\n",
    "    y_pred_at_k = np.take(y_pred, top_k_indices)\n",
    "\n",
    "    y_true_2d = y_true_at_k.reshape(1, -1)\n",
    "    y_pred_2d = y_pred_at_k.reshape(1, -1)\n",
    "\n",
    "    ndcg = ndcg_score(y_true_2d, y_pred_2d, k=k)\n",
    "    return ndcg\n",
    "\n",
    "def calculate_user_ndcg_for_dataset(df, k=20):\n",
    "    ndcg_scores = {}\n",
    "    grouped = df.groupby('user_id')\n",
    "    for user_id, group in grouped:\n",
    "        y_true = group['relevance'].values\n",
    "        y_pred = group['score'].values\n",
    "        ndcg = calculate_ndcg_at_k(y_true, y_pred, k)\n",
    "        ndcg_scores[user_id] = ndcg\n",
    "    return ndcg_scores\n",
    "\n",
    "def calculate_mean_ndcg(ndcg_scores_dict):\n",
    "    if not ndcg_scores_dict:\n",
    "        return 0.0\n",
    "    return np.mean(list(ndcg_scores_dict.values()))\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not XGB_AVAILABLE:\n",
    "        print(\"XGBoost is not available. Please install it to use this ensemble.\")\n",
    "        return\n",
    "        \n",
    "    seed_everything()\n",
    "\n",
    "    # Load\n",
    "    train_df, cand_df, u_meta, b_meta, desc_df = load_and_prep()\n",
    "\n",
    "    bert_embs = compute_bert_embeddings(desc_df)\n",
    "\n",
    "    # Find Best Iterations (Single Run) & Calculate NDCG ---\n",
    "    print(\"Finding optimal iterations and calculating NDCG...\")\n",
    "    train_df_sorted = train_df.sort_values(Constants.COL_TIMESTAMP)\n",
    "    split_idx = int(len(train_df_sorted) * (1 - Config.VAL_SIZE_RATIO))\n",
    "    train_part = train_df_sorted.iloc[:split_idx].copy()\n",
    "    val_part = train_df_sorted.iloc[split_idx:].copy()\n",
    "\n",
    "    svd_data_val = train_svd_model(train_part)\n",
    "    u_prof_val, _ = calculate_user_bert_profiles(train_part, bert_embs)\n",
    "\n",
    "    all_books = b_meta[Constants.COL_BOOK_ID].unique()\n",
    "    train_part_neg = generate_negatives(train_part, all_books)\n",
    "    val_part_neg = generate_negatives(val_part, all_books)\n",
    "\n",
    "    train_feat, cat_cols = build_features(train_part_neg, u_meta, b_meta, desc_df, svd_data_val, bert_embs, train_part,\n",
    "                                          u_prof_val)\n",
    "    val_feat, _ = build_features(val_part_neg, u_meta, b_meta, desc_df, svd_data_val, bert_embs, train_part, u_prof_val)\n",
    "\n",
    "    # Sort & Clean\n",
    "    train_feat = train_feat.sort_values(Constants.COL_USER_ID).reset_index(drop=True)\n",
    "    val_feat = val_feat.sort_values(Constants.COL_USER_ID).reset_index(drop=True)\n",
    "\n",
    "    drop_cols = [Constants.COL_USER_ID, Constants.COL_BOOK_ID, Constants.COL_RELEVANCE, Constants.COL_HAS_READ,\n",
    "                 Constants.COL_TIMESTAMP,\n",
    "                 \"description\", \"als_weight\", \"weight\", \"title\", \"author_name\", \"image_url\", \"book_id_list\"]\n",
    "\n",
    "    X_tr = train_feat.drop(columns=[c for c in drop_cols if c in train_feat.columns], errors='ignore')\n",
    "    X_val = val_feat.drop(columns=[c for c in drop_cols if c in val_feat.columns], errors='ignore')\n",
    "\n",
    "    real_cats = [c for c in cat_cols if c in X_tr.columns]\n",
    "    X_tr_clean = clean_data_for_models(X_tr.copy(), real_cats)\n",
    "    X_val_clean = clean_data_for_models(X_val.copy(), real_cats)\n",
    "\n",
    "    # Prepare CatBoost pools\n",
    "    train_pool = Pool(data=X_tr_clean, label=train_feat[Constants.COL_RELEVANCE], \n",
    "                      group_id=train_feat[Constants.COL_USER_ID],\n",
    "                      cat_features=real_cats)\n",
    "    val_pool = Pool(data=X_val_clean, label=val_feat[Constants.COL_RELEVANCE], \n",
    "                    group_id=val_feat[Constants.COL_USER_ID],\n",
    "                    cat_features=real_cats)\n",
    "\n",
    "    # --- Calculate NDCG for CatBoost ---\n",
    "    model = CatBoostRanker(**Config.CB_PARAMS)\n",
    "    model.fit(train_pool, eval_set=val_pool)\n",
    "    best_cb_iter = model.best_iteration_\n",
    "    \n",
    "    cb_val_preds = model.predict(X_val_clean)\n",
    "    val_with_cb_preds = val_feat.copy()\n",
    "    val_with_cb_preds['score'] = cb_val_preds\n",
    "    cb_ndcg_scores = calculate_user_ndcg_for_dataset(val_with_cb_preds, k=20)\n",
    "    mean_cb_ndcg = calculate_mean_ndcg(cb_ndcg_scores)\n",
    "    print(f\">>> CatBoost - Best Iteration: {best_cb_iter}, Mean NDCG@20 on Val: {mean_cb_ndcg:.4f}\")\n",
    "\n",
    "    # --- Calculate NDCG for XGBoost ---\n",
    "   \n",
    "    X_tr_xgb = X_tr_clean.select_dtypes(include=[np.number])\n",
    "    X_val_xgb = X_val_clean.select_dtypes(include=[np.number])\n",
    "    \n",
    "    group_train = prepare_group_info(train_feat[Constants.COL_USER_ID].values)\n",
    "    group_val = prepare_group_info(val_feat[Constants.COL_USER_ID].values)\n",
    "    \n",
    "    xgb_params_temp = Config.XGB_PARAMS.copy()\n",
    "    xgb_params_temp['n_estimators'] = min(best_cb_iter*2, 2000)\n",
    "    \n",
    "    xgb_model = xgb.XGBRanker(**xgb_params_temp)\n",
    "    xgb_model.fit(\n",
    "        X_tr_xgb, train_feat[Constants.COL_RELEVANCE].values,\n",
    "        group=group_train,\n",
    "        eval_set=[(X_val_xgb, val_feat[Constants.COL_RELEVANCE].values)],\n",
    "        eval_group=[group_val],\n",
    "        verbose=0\n",
    "    )\n",
    "    best_xgb_iter = xgb_model.best_iteration\n",
    "    \n",
    "    xgb_val_preds = xgb_model.predict(X_val_xgb)\n",
    "    val_with_xgb_preds = val_feat.copy()\n",
    "    val_with_xgb_preds['score'] = xgb_val_preds\n",
    "    xgb_ndcg_scores = calculate_user_ndcg_for_dataset(val_with_xgb_preds, k=20)\n",
    "    mean_xgb_ndcg = calculate_mean_ndcg(xgb_ndcg_scores)\n",
    "    print(f\">>> XGBoost - Best Iteration: {best_xgb_iter}, Mean NDCG@20 on Val: {mean_xgb_ndcg:.4f}\")\n",
    "\n",
    "    del train_part, val_part, train_feat, val_feat, train_pool, val_pool, model, xgb_model\n",
    "    gc.collect()\n",
    "\n",
    "    # --- STEP 2: ENSEMBLE REFIT ---\n",
    "    print(\"\\n\" + \"=\" * 30 + \" ENSEMBLE TRAINING \" + \"=\" * 30)\n",
    "\n",
    "    svd_data_full = train_svd_model(train_df)\n",
    "    u_prof_full, _ = calculate_user_bert_profiles(train_df, bert_embs)\n",
    "\n",
    "    train_full_neg = generate_negatives(train_df, all_books)\n",
    "    train_full_feat, _ = build_features(train_full_neg, u_meta, b_meta, desc_df, svd_data_full, bert_embs, train_df,\n",
    "                                        u_prof_full)\n",
    "    train_full_feat = train_full_feat.sort_values(Constants.COL_USER_ID).reset_index(drop=True)\n",
    "\n",
    "    X_full = train_full_feat.drop(columns=[c for c in drop_cols if c in train_full_feat.columns], errors='ignore')\n",
    "    X_full_clean = clean_data_for_models(X_full.copy(), real_cats)\n",
    "\n",
    "    cand_exp = expand_candidates(cand_df)\n",
    "    cand_feat, _ = build_features(cand_exp, u_meta, b_meta, desc_df, svd_data_full, bert_embs, train_df, u_prof_full)\n",
    "    X_test = cand_feat.drop(columns=[Constants.COL_USER_ID, Constants.COL_BOOK_ID], errors='ignore')\n",
    "    X_test = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns], errors='ignore')\n",
    "    for f in X_full_clean.columns:\n",
    "        if f not in X_test.columns: X_test[f] = 0\n",
    "    X_test = X_test[X_full_clean.columns]\n",
    "    X_test_clean = clean_data_for_models(X_test.copy(), real_cats)\n",
    "\n",
    "    # ✅ FIXED: Use include=[np.number] for XGBoost full and test sets\n",
    "    X_full_xgb = X_full_clean.select_dtypes(include=[np.number])\n",
    "    X_test_xgb = X_test_clean.select_dtypes(include=[np.number])\n",
    "    group_full = prepare_group_info(train_full_feat[Constants.COL_USER_ID].values)\n",
    "\n",
    "    cb_final_scores = np.zeros(len(X_test_clean))\n",
    "    xgb_final_scores = np.zeros(len(X_test_xgb))\n",
    "\n",
    "    for i, seed in enumerate(Config.SEEDS):\n",
    "        print(f\"Training Ensemble Models {i + 1}/{len(Config.SEEDS)} with seed {seed}...\")\n",
    "        \n",
    "        # CatBoost\n",
    "        cb_params = Config.CB_PARAMS.copy()\n",
    "        cb_params['iterations'] = best_cb_iter\n",
    "        cb_params['random_seed'] = seed\n",
    "\n",
    "        cb_m = CatBoostRanker(**cb_params)\n",
    "        cb_pool = Pool(data=X_full_clean, label=train_full_feat[Constants.COL_RELEVANCE],\n",
    "                       group_id=train_full_feat[Constants.COL_USER_ID], cat_features=real_cats)\n",
    "        cb_m.fit(cb_pool)\n",
    "        cb_preds = cb_m.predict(X_test_clean)\n",
    "        cb_final_scores += cb_preds\n",
    "        cb_m.save_model(Config.MODEL_DIR / f\"catboost_seed_{seed}.cbm\")\n",
    "        del cb_m, cb_pool\n",
    "        gc.collect()\n",
    "\n",
    "        # XGBoost\n",
    "        # XGBoost\n",
    "        xgb_params = Config.XGB_PARAMS.copy()\n",
    "        xgb_params['n_estimators'] = best_xgb_iter\n",
    "        xgb_params['random_state'] = seed\n",
    "        # Remove early stopping since no eval_set is provided in final training\n",
    "        xgb_params.pop('early_stopping_rounds', None)\n",
    "\n",
    "        xgb_m = xgb.XGBRanker(**xgb_params)\n",
    "        xgb_m.fit(X_full_xgb, train_full_feat[Constants.COL_RELEVANCE].values, group=group_full)\n",
    "        xgb_preds = xgb_m.predict(X_test_xgb)\n",
    "        xgb_final_scores += xgb_preds\n",
    "        xgb_m.save_model(str(Config.MODEL_DIR / f\"xgboost_seed_{seed}.json\"))\n",
    "        del xgb_m\n",
    "        gc.collect()\n",
    "\n",
    "    cb_avg_scores = cb_final_scores / len(Config.SEEDS)\n",
    "    xgb_avg_scores = xgb_final_scores / len(Config.SEEDS)\n",
    "    final_ensemble_scores = 0.5 * cb_avg_scores + 0.5 * xgb_avg_scores\n",
    "    cand_feat['score'] = final_ensemble_scores\n",
    "\n",
    "    cand_feat = cand_feat.sort_values([Constants.COL_USER_ID, 'score'], ascending=[True, False])\n",
    "    top_20 = cand_feat.groupby(Constants.COL_USER_ID).head(20)\n",
    "    sub = top_20.groupby(Constants.COL_USER_ID)[Constants.COL_BOOK_ID].apply(\n",
    "        lambda x: \",\".join(map(str, x))).reset_index()\n",
    "    sub.columns = [Constants.COL_USER_ID, Constants.COL_BOOK_ID_LIST]\n",
    "\n",
    "    targets = pd.read_csv(Config.RAW_DATA_DIR / Constants.TARGETS_FILENAME)\n",
    "    final_sub = targets.merge(sub, on=Constants.COL_USER_ID, how='left').fillna(\"\")\n",
    "\n",
    "    out = Config.SUBMISSION_DIR / \"sunny_xgb_cb.csv\"\n",
    "    final_sub.to_csv(out, index=False)\n",
    "    print(f\"Ensemble Done! Saved to {out}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
